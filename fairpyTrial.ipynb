{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install required libraries\n",
    "%pip install --upgrade pip\n",
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "%pip install tensorflow scikit-learn pandas numpy matplotlib seaborn sentencepiece transformers accelerate huggingface_hub bitsandbytes diffusers safetensors xformers peft wordcloud textblob aif360 datasets requests nltk pillow scikit-learn vaderSentiment\n",
    "\n",
    "# Install additional tools and model-specific packages\n",
    "%pip install git+https://github.com/openai/CLIP.git\n",
    "%pip install ftfy regex tqdm ninja\n",
    "!python -m spacy download en_core_web_sm\n",
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "import gc\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from wordcloud import WordCloud\n",
    "from textblob import TextBlob\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    CLIPProcessor,\n",
    "    CLIPModel,\n",
    "    BlipProcessor,\n",
    "    BlipForConditionalGeneration,\n",
    "    BitsAndBytesConfig,\n",
    "    EarlyStoppingCallback,\n",
    "    pipeline\n",
    ")\n",
    "from diffusers import DiffusionPipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import PCA, LatentDirichletAllocation\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.stats import ttest_ind\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download NLTK stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Import PEFT for fine-tuning models\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "# Check versions of critical libraries\n",
    "print(f\"BitsAndBytes version: {bnb.__version__}\")\n",
    "\n",
    "# Additional NLP setup\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "# System and Utility Libraries\n",
    "import os\n",
    "import gc\n",
    "import json\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Data Processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import PCA, LatentDirichletAllocation\n",
    "from sklearn.manifold import TSNE\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# NLP and Transformers\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    CLIPProcessor,\n",
    "    CLIPModel,\n",
    "    BlipProcessor,\n",
    "    BlipForConditionalGeneration,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    pipeline,\n",
    "    BitsAndBytesConfig,\n",
    "    EarlyStoppingCallback,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# BitsAndBytes for Model Optimization\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Diffusers\n",
    "from diffusers.utils import pt_to_pil\n",
    "\n",
    "# Image Processing\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "# SpaCy\n",
    "import spacy\n",
    "\n",
    "from huggingface_hub import login, notebook_login  # Add this import\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import gc\n",
    "import psutil\n",
    "import tensorflow as tf  # Add this import\n",
    "import copy  # Add this import\n",
    "import requests  # Add this import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install six\n",
    "%pip install --upgrade urllib3 requests pytorch-lightning\n",
    "import torch\n",
    "import os\n",
    "import tensorflow as tf\n",
    "%pip install psutil\n",
    "import psutil\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"Number of GPUs available: {num_gpus}\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")\n",
    "\n",
    "# Set the device to GPU 0\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Set the device to GPU 1\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "print(\"GPU available:\", torch.cuda.is_available())\n",
    "print(\"Device:\", torch.cuda.current_device())\n",
    "ram_gb = psutil.virtual_memory().total / 1e9  # Use psutil.virtual_memory()\n",
    "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
    "\n",
    "if ram_gb < 20:\n",
    "  print('Not using a high-RAM runtime')\n",
    "else:\n",
    "  print('You are using a high-RAM runtime!')\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"  # as nvida gpu is gpu1, while intel gpu is gpu0\n",
    "os.environ[\"PYTHONHASHSEED\"]=\"1\"\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    print('Using GPU')\n",
    "else:\n",
    "    print('Using CPU only')\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login, notebook_login\n",
    "\n",
    "# Use your Hugging Face token\n",
    "token = \"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\"  # Replace with the token you just created\n",
    "\n",
    "# Login to Hugging Face\n",
    "login(token=token)\n",
    "\n",
    "# For notebook login (if needed)\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Use a pipeline as a high-level helper to run this model\n",
    "pipe = pipeline(\"text-generation\", model=\"meta-llama/Llama-3.2-1B\")\n",
    "\n",
    "# Load the tokenizer and model using your token\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\", use_auth_token=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\", use_auth_token=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset from the given file path\n",
    "file_path = r\"C:\\Users\\rawan\\OneDrive\\Desktop\\thesisModify\\Processed_Most_Common_Term_By_Bias.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows to verify the structure\n",
    "print(data.head())\n",
    "# Display column names\n",
    "print(\"Columns in the dataset:\")\n",
    "print(data.columns)\n",
    "\n",
    "# Verify the result\n",
    "print(\"Updated columns in the dataset:\")\n",
    "print(data.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to keep the \"Most Common Term\" column\n",
    "print(\"Columns in the dataset before dropping any:\")\n",
    "print(data.columns)\n",
    "\n",
    "# Avoid dropping the column and proceed with analysis\n",
    "# Remove this line if you don't want to drop columns:\n",
    "# data = data.iloc[:, :-1]\n",
    "\n",
    "print(\"Columns retained in the dataset:\")\n",
    "print(data.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with missing responses\n",
    "data = data.dropna(subset=[col for col in data.columns if 'Model Response' in col])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install fairlearn\n",
    "%pip install themis-ml\n",
    "%pip install witwidget\n",
    "%pip install fairml\n",
    "%pip install shap\n",
    "%pip install lime\n",
    "%pip install fairkit-learn\n",
    "import pandas as pd\n",
    "from fairlearn.metrics import demographic_parity_difference\n",
    "from fairlearn.postprocessing import ThresholdOptimizer\n",
    "from textblob import TextBlob\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.metrics import demographic_parity_difference\n",
    "\n",
    "# Use 'Most Common Term' as predictions\n",
    "data['All Responses'] = data['Most Common Term']\n",
    "\n",
    "# Drop rows with missing responses\n",
    "data = data.dropna(subset=['All Responses'])\n",
    "\n",
    "# Evaluate fairness using demographic parity difference\n",
    "dp_difference = demographic_parity_difference(\n",
    "    y_true=data['Original Prompt'],  # Treat Original Prompt as baseline truth\n",
    "    y_pred=data['All Responses'],   # Use 'Most Common Term' as predictions\n",
    "    sensitive_features=data['Prompt Type']  # Protected attribute: Prompt Type\n",
    ")\n",
    "print(\"Demographic Parity Difference:\", dp_difference)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.metrics import demographic_parity_difference\n",
    "\n",
    "# Categorize responses as positive or not\n",
    "data['All Responses Category'] = data['All Responses'].apply(\n",
    "    lambda x: 1 if 'positive' in str(x).lower() else 0\n",
    ")\n",
    "\n",
    "# Evaluate fairness using demographic parity difference\n",
    "dp_difference = demographic_parity_difference(\n",
    "    y_true=data['Original Prompt'],  # This is baseline truth, though categorical might not be perfect\n",
    "    y_pred=data['All Responses Category'],  # Categorical predictions\n",
    "    sensitive_features=data['Prompt Type']  # Protected attribute\n",
    ")\n",
    "\n",
    "print(\"Demographic Parity Difference:\", dp_difference)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure 'All Responses' has no missing values to avoid errors\n",
    "data['All Responses'] = data['All Responses'].fillna('')\n",
    "\n",
    "# Calculate sentiment polarity for each response\n",
    "data['Sentiment'] = data['All Responses'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "\n",
    "# Calculate average sentiment per Prompt Type\n",
    "average_sentiment = data.groupby('Prompt Type')['Sentiment'].mean()\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(8, 6))  # Set figure size for better readability\n",
    "average_sentiment.plot(\n",
    "    kind='bar',\n",
    "    color='coral',\n",
    "    title='Average Sentiment by Prompt Type',\n",
    "    edgecolor='black'  # Add borders to bars\n",
    ")\n",
    "\n",
    "# Add labels and title\n",
    "plt.ylabel('Average Sentiment', fontsize=12)\n",
    "plt.xlabel('Prompt Type', fontsize=12)\n",
    "plt.title('Average Sentiment by Prompt Type', fontsize=14)\n",
    "plt.xticks(rotation=45, fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()  # Adjust layout to prevent clipping\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique values in All Responses Category:\", data['All Responses Category'].unique())\n",
    "print(\"Value counts in All Responses Category:\")\n",
    "print(data['All Responses Category'].value_counts())\n",
    "print(data['All Responses'].head(10))\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Classify All Responses using sentiment polarity\n",
    "data['All Responses Category'] = data['All Responses'].apply(\n",
    "    lambda x: 1 if TextBlob(str(x)).sentiment.polarity > 0 else 0\n",
    ")\n",
    "\n",
    "# Recheck unique values and counts\n",
    "print(\"Unique values in All Responses Category after sentiment-based classification:\", data['All Responses Category'].unique())\n",
    "print(\"Value counts in All Responses Category after fixing:\")\n",
    "print(data['All Responses Category'].value_counts())\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Separate majority and minority classes\n",
    "majority = data[data['All Responses Category'] == 0]\n",
    "minority = data[data['All Responses Category'] == 1]\n",
    "\n",
    "# Oversample minority class\n",
    "minority_upsampled = resample(\n",
    "    minority,\n",
    "    replace=True,  # Sample with replacement\n",
    "    n_samples=len(majority),  # Match majority size\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Combine majority and oversampled minority classes\n",
    "data_balanced = pd.concat([majority, minority_upsampled])\n",
    "\n",
    "# Shuffle the data\n",
    "data_balanced = data_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "# Prepare features (X) and labels (y) from balanced dataset\n",
    "X = data_balanced['Original Prompt']\n",
    "y = data_balanced['All Responses Category']\n",
    "\n",
    "# Convert text features to numerical format using CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X_transformed_sparse = vectorizer.fit_transform(X)\n",
    "X_transformed = X_transformed_sparse.toarray()\n",
    "\n",
    "# Split into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test, sensitive_train, sensitive_test = train_test_split(\n",
    "    X_transformed, \n",
    "    y, \n",
    "    data_balanced['Prompt Type'], \n",
    "    test_size=0.3, \n",
    "    random_state=42, \n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "# Train the Threshold Optimizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from fairlearn.postprocessing import ThresholdOptimizer\n",
    "\n",
    "base_estimator = LogisticRegression(max_iter=1000)\n",
    "\n",
    "post_processor = ThresholdOptimizer(\n",
    "    estimator=base_estimator,\n",
    "    constraints=\"demographic_parity\"\n",
    ")\n",
    "\n",
    "post_processor.fit(\n",
    "    X=X_train,\n",
    "    y=y_train,\n",
    "    sensitive_features=sensitive_train\n",
    ")\n",
    "\n",
    "# Adjust predictions on the test set\n",
    "adjusted_predictions = post_processor.predict(\n",
    "    X=X_test,\n",
    "    sensitive_features=sensitive_test\n",
    ")\n",
    "\n",
    "# Evaluate positive response rates\n",
    "test_data = pd.DataFrame(X_test, columns=vectorizer.get_feature_names_out())\n",
    "test_data['Prompt Type'] = sensitive_test.values\n",
    "test_data['Adjusted Responses'] = adjusted_predictions\n",
    "\n",
    "adjusted_positive_rate = test_data.groupby('Prompt Type')['Adjusted Responses'].mean()\n",
    "print(\"Adjusted Positive Response Rates:\")\n",
    "print(adjusted_positive_rate)\n",
    "\n",
    "# Plot Positive Response Rates\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "adjusted_positive_rate.plot(kind='bar', color='green', title='Adjusted Positive Response Rates by Prompt Type')\n",
    "plt.ylabel('Adjusted Positive Response Rate')\n",
    "plt.xlabel('Prompt Type')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "test_data.to_csv(\"adjusted_test_data.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.metrics import equalized_odds_difference\n",
    "\n",
    "# Compute Equalized Odds Difference\n",
    "eo_difference = equalized_odds_difference(\n",
    "    y_true=y_test,  # Use the true labels from the test set\n",
    "    y_pred=adjusted_predictions,  # Use the adjusted predictions\n",
    "    sensitive_features=sensitive_test  # Sensitive attribute from the test set\n",
    ")\n",
    "\n",
    "print(\"Equalized Odds Difference:\", eo_difference)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Accuracy of original predictions (before fairness adjustment)\n",
    "original_predictions = post_processor.estimator_.predict(X_test)\n",
    "original_accuracy = accuracy_score(y_test, original_predictions)\n",
    "print(\"Original Accuracy:\", original_accuracy)\n",
    "\n",
    "# Accuracy of adjusted predictions (after fairness adjustment)\n",
    "adjusted_accuracy = accuracy_score(y_test, adjusted_predictions)\n",
    "print(\"Adjusted Accuracy:\", adjusted_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Ensure Adjusted Responses are available\n",
    "# Map adjusted predictions back to their corresponding Prompt Type in `data_balanced`\n",
    "data_balanced['Adjusted Responses'] = np.nan  # Initialize with NaN\n",
    "data_balanced.loc[y_test.index, 'Adjusted Responses'] = adjusted_predictions  # Align predictions with the test set\n",
    "\n",
    "# Calculate before and after rates\n",
    "before_rates = data_balanced.groupby('Prompt Type')['All Responses Category'].mean()\n",
    "after_rates = data_balanced.groupby('Prompt Type')['Adjusted Responses'].mean()\n",
    "\n",
    "# Plot comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 6))  # Adjust figure size for readability\n",
    "x = np.arange(len(before_rates.index))  # x-axis positions\n",
    "\n",
    "bar_width = 0.35  # Width of each bar\n",
    "\n",
    "# Plot before and after bars side by side\n",
    "ax.bar(x - bar_width/2, before_rates, bar_width, label='Before Mitigation', alpha=0.7, color='blue')\n",
    "ax.bar(x + bar_width/2, after_rates, bar_width, label='After Mitigation', alpha=0.7, color='orange')\n",
    "\n",
    "# Add labels and title\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(before_rates.index, rotation=45, fontsize=10)\n",
    "ax.set_title('Positive Response Rates Before and After Mitigation', fontsize=14)\n",
    "ax.set_ylabel('Positive Response Rate', fontsize=12)\n",
    "ax.set_xlabel('Prompt Type', fontsize=12)\n",
    "ax.legend()\n",
    "\n",
    "# Adjust layout and display\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape of X_transformed: {X_transformed.shape}\")\n",
    "print(f\"Length of y: {len(y)}\")\n",
    "print(f\"Length of sensitive_features: {len(data['Prompt Type'])}\")\n",
    "# Filter out rows with missing values in any relevant column\n",
    "filtered_data = data.dropna(subset=['Original Prompt', 'All Responses Category', 'Prompt Type'])\n",
    "\n",
    "# Regenerate X_transformed, y, and sensitive_features\n",
    "X = filtered_data['Original Prompt']\n",
    "y = filtered_data['All Responses Category']\n",
    "sensitive_features = filtered_data['Prompt Type']\n",
    "\n",
    "# Recompute X_transformed\n",
    "X_transformed_sparse = vectorizer.fit_transform(X)\n",
    "X_transformed = X_transformed_sparse.toarray()\n",
    "# Initialize ThresholdOptimizer\n",
    "post_processor = ThresholdOptimizer(\n",
    "    estimator=base_estimator,\n",
    "    constraints=\"equalized_odds\"\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "post_processor.fit(\n",
    "    X=X_transformed,\n",
    "    y=y,\n",
    "    sensitive_features=sensitive_features\n",
    ")\n",
    "\n",
    "# Adjust predictions\n",
    "filtered_data['Adjusted Responses EO'] = post_processor.predict(\n",
    "    X=X_transformed,\n",
    "    sensitive_features=sensitive_features\n",
    ")\n",
    "eo_difference_after = equalized_odds_difference(\n",
    "    y_true=y,\n",
    "    y_pred=filtered_data['Adjusted Responses EO'],\n",
    "    sensitive_features=sensitive_features\n",
    ")\n",
    "print(\"Equalized Odds Difference (After Mitigation):\", eo_difference_after)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate positive response rate by Prompt Type\n",
    "positive_rate = filtered_data.groupby('Prompt Type')['All Responses Category'].mean()\n",
    "print(\"\\nPositive Response Rates (Before Mitigation):\")\n",
    "for prompt_type, rate in positive_rate.items():\n",
    "    print(f\"{prompt_type}: {rate:.2f}\")\n",
    "\n",
    "# Calculate average sentiment score by Prompt Type\n",
    "average_sentiment = filtered_data.groupby('Prompt Type')['Sentiment'].mean()\n",
    "print(\"\\nAverage Sentiment Scores by Prompt Type:\")\n",
    "for prompt_type, sentiment in average_sentiment.items():\n",
    "    print(f\"{prompt_type}: {sentiment:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from fairlearn.postprocessing import ThresholdOptimizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure data alignment and remove missing values\n",
    "filtered_data = data.dropna(subset=['Original Prompt', 'All Responses Category', 'Prompt Type'])\n",
    "\n",
    "# Prepare features (X) and labels (y)\n",
    "X = filtered_data['Original Prompt']  # Input features\n",
    "y = filtered_data['All Responses Category']  # Binary responses\n",
    "sensitive_features = filtered_data['Prompt Type']  # Sensitive feature\n",
    "\n",
    "# Convert text into numerical format using CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X_transformed = vectorizer.fit_transform(X).toarray()\n",
    "\n",
    "# Initialize the base estimator\n",
    "base_estimator = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# Train the ThresholdOptimizer with Equalized Odds constraints\n",
    "post_processor = ThresholdOptimizer(\n",
    "    estimator=base_estimator,\n",
    "    constraints=\"equalized_odds\"\n",
    ")\n",
    "post_processor.fit(\n",
    "    X=X_transformed,\n",
    "    y=y,\n",
    "    sensitive_features=sensitive_features\n",
    ")\n",
    "\n",
    "# Adjust predictions using Equalized Odds constraints\n",
    "filtered_data['Adjusted Responses EO'] = post_processor.predict(\n",
    "    X=X_transformed,\n",
    "    sensitive_features=sensitive_features\n",
    ")\n",
    "\n",
    "# Save Adjusted Responses EO to the dataset\n",
    "print(\"Adjusted Responses EO added to the dataset.\")\n",
    "\n",
    "# Save the updated dataset to a CSV file\n",
    "output_path = r\"C:\\Users\\rawan\\OneDrive\\Desktop\\thesisModify\\Fairness_Results_With_EO.csv\"\n",
    "filtered_data.to_csv(output_path, index=False)\n",
    "print(f\"Processed data with Adjusted Responses EO saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.metrics import equalized_odds_difference, demographic_parity_difference\n",
    "\n",
    "# Evaluate Equalized Odds Difference\n",
    "eo_difference_after = equalized_odds_difference(\n",
    "    y_true=filtered_data['All Responses Category'],  # True labels\n",
    "    y_pred=filtered_data['Adjusted Responses EO'],  # Adjusted predictions\n",
    "    sensitive_features=filtered_data['Prompt Type']  # Sensitive feature\n",
    ")\n",
    "print(\"\\nEqualized Odds Difference (After Mitigation):\", eo_difference_after)\n",
    "\n",
    "# Evaluate Demographic Parity Difference (optional, for comparison)\n",
    "dp_difference_after = demographic_parity_difference(\n",
    "    y_true=filtered_data['All Responses Category'],  # True labels\n",
    "    y_pred=filtered_data['Adjusted Responses EO'],  # Adjusted predictions\n",
    "    sensitive_features=filtered_data['Prompt Type']  # Sensitive feature\n",
    ")\n",
    "print(\"Demographic Parity Difference (After Mitigation):\", dp_difference_after)\n",
    "\n",
    "# Compare Positive Response Rates (Before and After)\n",
    "positive_rate_before = filtered_data.groupby('Prompt Type')['All Responses Category'].mean()\n",
    "positive_rate_after = filtered_data.groupby('Prompt Type')['Adjusted Responses EO'].mean()\n",
    "\n",
    "print(\"\\nPositive Response Rates (Before Mitigation):\")\n",
    "print(positive_rate_before)\n",
    "\n",
    "print(\"\\nPositive Response Rates (After Mitigation):\")\n",
    "print(positive_rate_after)\n",
    "\n",
    "# Optional: Save metrics to a CSV for reporting\n",
    "metrics_output_path = r\"C:\\Users\\rawan\\OneDrive\\Desktop\\thesisModify\\Fairness_Metrics.csv\"\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Prompt Type': positive_rate_before.index,\n",
    "    'Positive Rate Before': positive_rate_before.values,\n",
    "    'Positive Rate After': positive_rate_after.values,\n",
    "    'Equalized Odds Difference': [eo_difference_after] * len(positive_rate_before),\n",
    "    'Demographic Parity Difference': [dp_difference_after] * len(positive_rate_before)\n",
    "})\n",
    "metrics_df.to_csv(metrics_output_path, index=False)\n",
    "print(f\"Fairness metrics saved to: {metrics_output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Define the privileged group and set instance weights\n",
    "privileged_group = 'Age Bias'  # Replace with your privileged group\n",
    "data['Instance Weight'] = data['Prompt Type'].apply(\n",
    "    lambda x: 1.0 if x == privileged_group else 2.0  # Increase weight for unprivileged groups\n",
    ")\n",
    "\n",
    "# Prepare features (X) and labels (y)\n",
    "X = data['Original Prompt']  # Input textual features\n",
    "y = data['All Responses Category']  # Binary labels\n",
    "\n",
    "# Convert text to numerical format using CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X_transformed = vectorizer.fit_transform(X)\n",
    "\n",
    "# Train logistic regression model with instance weights\n",
    "model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "model.fit(X_transformed, y, sample_weight=data['Instance Weight'])\n",
    "\n",
    "# Predict and evaluate\n",
    "predictions = model.predict(X_transformed)\n",
    "accuracy = accuracy_score(y, predictions)\n",
    "print(f\"Accuracy with manual reweighting: {accuracy:.4f}\")\n",
    "\n",
    "# Group-wise accuracy for further analysis\n",
    "group_accuracy = data.copy()\n",
    "group_accuracy['Predictions'] = predictions\n",
    "group_accuracy['Correct'] = (group_accuracy['Predictions'] == group_accuracy['All Responses Category']).astype(int)\n",
    "\n",
    "group_accuracy_by_prompt = group_accuracy.groupby('Prompt Type')['Correct'].mean()\n",
    "print(\"\\nGroup-wise Accuracy:\")\n",
    "for group, acc in group_accuracy_by_prompt.items():\n",
    "    print(f\"{group}: {acc:.4f}\")\n",
    "\n",
    "# Save results for further analysis\n",
    "output_path = r\"C:\\Users\\rawan\\OneDrive\\Desktop\\thesisModify\\Weighted_Results.csv\"\n",
    "group_accuracy.to_csv(output_path, index=False)\n",
    "print(f\"Weighted results saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "# Define a pipeline for vectorization and classification\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer()),  # Use TF-IDF for text representation\n",
    "    ('classifier', LogisticRegression(max_iter=1000, random_state=42))  # Logistic Regression\n",
    "])\n",
    "\n",
    "# Fit the pipeline with instance weights\n",
    "pipeline.fit(\n",
    "    data['Original Prompt'],  # Input text\n",
    "    data['All Responses Category'],  # Target labels\n",
    "    classifier__sample_weight=data['Instance Weight']  # Instance weights\n",
    ")\n",
    "\n",
    "# Predict using the trained pipeline\n",
    "predictions = pipeline.predict(data['Original Prompt'])\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(data['All Responses Category'], predictions)\n",
    "print(f\"Pipeline accuracy with reweighting: {accuracy:.4f}\")\n",
    "\n",
    "# Group-wise accuracy for further analysis\n",
    "group_accuracy = data.copy()\n",
    "group_accuracy['Predictions'] = predictions\n",
    "group_accuracy['Correct'] = (group_accuracy['Predictions'] == group_accuracy['All Responses Category']).astype(int)\n",
    "\n",
    "group_accuracy_by_prompt = group_accuracy.groupby('Prompt Type')['Correct'].mean()\n",
    "print(\"\\nGroup-wise Accuracy:\")\n",
    "for group, acc in group_accuracy_by_prompt.items():\n",
    "    print(f\"{group}: {acc:.4f}\")\n",
    "\n",
    "# Save predictions for further analysis\n",
    "output_path = r\"C:\\Users\\rawan\\OneDrive\\Desktop\\thesisModify\\Pipeline_Weighted_Results.csv\"\n",
    "group_accuracy.to_csv(output_path, index=False)\n",
    "print(f\"Pipeline weighted results saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from fairlearn.reductions import ExponentiatedGradient, DemographicParity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "# Prepare features (X) and labels (y)\n",
    "X = data['Original Prompt']\n",
    "y = data['All Responses Category']\n",
    "\n",
    "# Convert text to numerical format using CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X_transformed_sparse = vectorizer.fit_transform(X)\n",
    "\n",
    "# Convert sparse matrix to dense array\n",
    "X_transformed = X_transformed_sparse.toarray()\n",
    "\n",
    "# Define base estimator\n",
    "estimator = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# Apply Fairlearn's ExponentiatedGradient with Demographic Parity constraint\n",
    "fair_model = ExponentiatedGradient(estimator, constraints=DemographicParity())\n",
    "fair_model.fit(X_transformed, y, sensitive_features=data['Prompt Type'])\n",
    "\n",
    "# Predict using the fairness-constrained model\n",
    "predictions = fair_model.predict(X_transformed)\n",
    "\n",
    "# Evaluate overall accuracy\n",
    "accuracy = accuracy_score(y, predictions)\n",
    "print(f\"Accuracy with ExponentiatedGradient: {accuracy:.4f}\")\n",
    "\n",
    "# Evaluate group-wise accuracy\n",
    "group_accuracy = data.copy()\n",
    "group_accuracy['Predictions'] = predictions\n",
    "group_accuracy['Correct'] = (group_accuracy['Predictions'] == group_accuracy['All Responses Category']).astype(int)\n",
    "\n",
    "group_accuracy_by_prompt = group_accuracy.groupby('Prompt Type')['Correct'].mean()\n",
    "print(\"\\nGroup-wise Accuracy:\")\n",
    "for group, acc in group_accuracy_by_prompt.items():\n",
    "    print(f\"{group}: {acc:.4f}\")\n",
    "\n",
    "# Save predictions and group analysis for further reporting\n",
    "output_path = r\"C:\\Users\\rawan\\OneDrive\\Desktop\\thesisModify\\Fairlearn_DemographicParity_Results.csv\"\n",
    "group_accuracy.to_csv(output_path, index=False)\n",
    "print(f\"Results with Demographic Parity saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Prepare the data\n",
    "majority = data[data['Prompt Type'] == 'Age Bias']  # Majority group (Age Bias)\n",
    "minority = data[data['Prompt Type'] != 'Age Bias']  # Minority groups (not Age Bias)\n",
    "\n",
    "# Oversample minority group to match the size of the majority group\n",
    "minority_oversampled = resample(\n",
    "    minority,\n",
    "    replace=True,  # Sample with replacement\n",
    "    n_samples=len(majority),  # Match majority size\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Combine the oversampled minority with the majority group to balance the data\n",
    "balanced_data = pd.concat([majority, minority_oversampled])\n",
    "\n",
    "# Extract features and labels from balanced dataset\n",
    "X_balanced = balanced_data['Original Prompt']\n",
    "y_balanced = balanced_data['All Responses Category']\n",
    "\n",
    "# Convert text data to numerical format using CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X_transformed_balanced = vectorizer.fit_transform(X_balanced)\n",
    "\n",
    "# Define and train the logistic regression model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_transformed_balanced, y_balanced)\n",
    "\n",
    "# Predict and evaluate the model on the balanced data\n",
    "predictions = model.predict(X_transformed_balanced)\n",
    "accuracy = accuracy_score(y_balanced, predictions)\n",
    "\n",
    "print(f\"Accuracy with balanced data: {accuracy}\")\n",
    "\n",
    "# Optionally, you can save the model and results or further analyze group-wise performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the dataset is filtered to match the predictions' length\n",
    "filtered_data = data.iloc[:len(predictions)]  # Adjust to match your prediction subset\n",
    "\n",
    "# Align y_true and sensitive_features with the predictions\n",
    "y_true_filtered = filtered_data['All Responses Category']\n",
    "sensitive_features_filtered = filtered_data['Prompt Type']\n",
    "\n",
    "# Now, y_true_filtered and sensitive_features_filtered should match the predictions' length\n",
    "print(f\"Length of filtered data: {len(filtered_data)}\")\n",
    "print(f\"Length of predictions: {len(predictions)}\")\n",
    "print(f\"Length of y_true_filtered: {len(y_true_filtered)}\")\n",
    "print(f\"Length of sensitive_features_filtered: {len(sensitive_features_filtered)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.metrics import demographic_parity_difference, equalized_odds_difference\n",
    "\n",
    "# Validate lengths to ensure alignment\n",
    "assert len(y_true_filtered) == len(predictions), \"Mismatch between true labels and predictions.\"\n",
    "assert len(y_true_filtered) == len(sensitive_features_filtered), \"Mismatch between labels and sensitive features.\"\n",
    "\n",
    "# Calculate fairness metrics\n",
    "dp_diff = demographic_parity_difference(\n",
    "    y_true=y_true_filtered,\n",
    "    y_pred=predictions,\n",
    "    sensitive_features=sensitive_features_filtered\n",
    ")\n",
    "eo_diff = equalized_odds_difference(\n",
    "    y_true=y_true_filtered,\n",
    "    y_pred=predictions,\n",
    "    sensitive_features=sensitive_features_filtered\n",
    ")\n",
    "\n",
    "# Display fairness metrics\n",
    "print(f\"Demographic Parity Difference: {dp_diff:.4f}\")\n",
    "print(f\"Equalized Odds Difference: {eo_diff:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure alignment between predictions and the filtered dataset\n",
    "filtered_data = data.iloc[:len(predictions)]  # Adjust to match predictions\n",
    "\n",
    "# Define sensitive attributes to analyze\n",
    "sensitive_attributes = ['Prompt Type']  # Add other sensitive attributes here, e.g., 'Age', 'Gender'\n",
    "\n",
    "# Initialize a dictionary to store metrics\n",
    "group_metrics = {}\n",
    "\n",
    "# Analyze each sensitive attribute individually\n",
    "for attribute in sensitive_attributes:\n",
    "    group_metrics[attribute] = {}\n",
    "    unique_groups = filtered_data[attribute].unique()\n",
    "\n",
    "    for group in unique_groups:\n",
    "        # Filter data for the current group\n",
    "        group_indices = filtered_data[attribute] == group\n",
    "        y_true_group = filtered_data.loc[group_indices, 'All Responses Category']\n",
    "        predictions_group = predictions[group_indices]\n",
    "\n",
    "        # Compute confusion matrix\n",
    "        cm = confusion_matrix(y_true_group, predictions_group)\n",
    "        tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "        tpr = tp / (tp + fn) if (tp + fn) > 0 else 0  # True Positive Rate\n",
    "        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0  # False Positive Rate\n",
    "        accuracy = accuracy_score(y_true_group, predictions_group)\n",
    "\n",
    "        # Store metrics for the current group\n",
    "        group_metrics[attribute][group] = {\n",
    "            \"True Positive Rate (TPR)\": tpr,\n",
    "            \"False Positive Rate (FPR)\": fpr,\n",
    "            \"Accuracy\": accuracy\n",
    "        }\n",
    "\n",
    "# Display metrics by each sensitive attribute\n",
    "print(\"\\nMetrics by Sensitive Attribute:\")\n",
    "for attribute, groups in group_metrics.items():\n",
    "    print(f\"\\nMetrics for {attribute}:\")\n",
    "    for group, metrics in groups.items():\n",
    "        print(f\"  {group}:\")\n",
    "        for metric, value in metrics.items():\n",
    "            print(f\"    {metric}: {value:.4f}\")\n",
    "\n",
    "# Save metrics for further reporting\n",
    "metrics_combined = pd.concat(\n",
    "    {attr: pd.DataFrame.from_dict(groups, orient='index') for attr, groups in group_metrics.items()},\n",
    "    axis=0\n",
    ")\n",
    "metrics_combined.to_csv(\"all_sensitive_attribute_metrics.csv\")\n",
    "print(\"\\nMetrics saved to: all_sensitive_attribute_metrics.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Length of y_true_filtered: {len(y_true_filtered)}\")\n",
    "print(f\"Length of predictions: {len(predictions)}\")\n",
    "print(f\"Length of sensitive_features_filtered: {len(sensitive_features_filtered)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Fairness metrics\n",
    "metrics = ['Demographic Parity Difference', 'Equalized Odds Difference']\n",
    "values = [dp_diff, eo_diff]\n",
    "\n",
    "# Create the bar chart\n",
    "plt.figure(figsize=(8, 6))  # Adjust figure size for better readability\n",
    "plt.bar(metrics, values, color=['skyblue', 'coral'], edgecolor='black', alpha=0.7)\n",
    "plt.title('Fairness Metrics After Applying Constraints', fontsize=14)\n",
    "plt.ylabel('Metric Value', fontsize=12)\n",
    "plt.xticks(fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.ylim(0, max(values) + 0.1)  # Add some space above the highest bar\n",
    "\n",
    "# Display the bar chart\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metrics\n",
    "metrics_result = {\n",
    "    'Demographic Parity Difference': dp_diff,\n",
    "    'Equalized Odds Difference': eo_diff\n",
    "}\n",
    "metrics_path = r\"C:\\Users\\rawan\\OneDrive\\Desktop\\thesisModify\\Fairness_Metrics.json\"\n",
    "\n",
    "import json\n",
    "with open(metrics_path, 'w') as f:\n",
    "    json.dump(metrics_result, f)\n",
    "\n",
    "print(f\"Fairness metrics saved to: {metrics_path}\")\n",
    "\n",
    "# Save the filtered dataset\n",
    "output_path = r\"C:\\Users\\rawan\\OneDrive\\Desktop\\thesisModify\\Filtered_Dataset.csv\"\n",
    "filtered_data.to_csv(output_path, index=False)\n",
    "print(f\"Filtered dataset saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "unprivileged_groups = [{'Prompt Type': 'Age Bias'}]\n",
    "privileged_groups = [{'Prompt Type': 'Gender Bias'}, {'Prompt Type': 'Race Bias'}, {'Prompt Type': 'Ethnic Bias'}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from aif360.algorithms.preprocessing import Reweighing\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "\n",
    "# Load the dataset\n",
    "data_path = \"Processed_Most_Common_Term_By_Bias.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Handle missing values\n",
    "if df.isna().any().any():\n",
    "    df = df.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "import pandas as pd\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.algorithms.preprocessing import Reweighing\n",
    "\n",
    "# Load the dataset\n",
    "data_path = \"Processed_Most_Common_Term_By_Bias.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Drop unnecessary non-numerical columns\n",
    "df = df.drop(columns=['Original Prompt'], errors='ignore')  # Drop textual data\n",
    "\n",
    "# Encode categorical columns into numerical format\n",
    "df['Prompt Type'] = df['Prompt Type'].astype('category').cat.codes  # Encode 'Prompt Type'\n",
    "df['Most Common Term'] = df['Most Common Term'].astype('category').cat.codes  # Encode 'Most Common Term'\n",
    "\n",
    "# Ensure binary labels with at least one instance of each class\n",
    "most_frequent_term = df['Most Common Term'].mode()[0]  # Most frequent term\n",
    "df['Most Common Term'] = (df['Most Common Term'] == most_frequent_term).astype(int)\n",
    "\n",
    "# Separate majority and minority classes\n",
    "majority_class = df[df['Most Common Term'] == 0]\n",
    "minority_class = df[df['Most Common Term'] == 1]\n",
    "\n",
    "# Oversample the minority class\n",
    "minority_oversampled = resample(\n",
    "    minority_class,\n",
    "    replace=True,          # Sample with replacement\n",
    "    n_samples=len(majority_class),  # Match the majority class size\n",
    "    random_state=42        # Reproducible results\n",
    ")\n",
    "\n",
    "# Combine the oversampled minority class with the majority class\n",
    "balanced_df = pd.concat([majority_class, minority_oversampled])\n",
    "\n",
    "# Shuffle the dataset\n",
    "balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Check the distribution\n",
    "print(balanced_df['Most Common Term'].value_counts())\n",
    "\n",
    "# Define unprivileged and privileged groups\n",
    "unprivileged_groups = [{'Prompt Type': 0}]  # Example: 'Age Bias' encoded as 0\n",
    "privileged_groups = [{'Prompt Type': 1}]  # Example: 'Other Bias' encoded as 1\n",
    "\n",
    "# Convert to BinaryLabelDataset\n",
    "dataset = BinaryLabelDataset(\n",
    "    df=balanced_df,\n",
    "    label_names=['Most Common Term'],\n",
    "    protected_attribute_names=['Prompt Type'],\n",
    "    favorable_label=1.0,\n",
    "    unfavorable_label=0.0\n",
    ")\n",
    "\n",
    "# Apply Reweighing\n",
    "reweigher = Reweighing(\n",
    "    unprivileged_groups=unprivileged_groups,\n",
    "    privileged_groups=privileged_groups\n",
    ")\n",
    "balanced_data = reweigher.fit_transform(dataset)\n",
    "\n",
    "# View transformed data\n",
    "balanced_data_df, _ = balanced_data.convert_to_dataframe()\n",
    "print(balanced_data_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(balanced_df.groupby('Prompt Type')['Most Common Term'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check unique values in 'Prompt Type'\n",
    "print(balanced_df['Prompt Type'].unique())\n",
    "\n",
    "# Define unprivileged and privileged groups based on the data\n",
    "unprivileged_groups = [{'Prompt Type': balanced_df['Prompt Type'].min()}]  # Example: smallest group\n",
    "privileged_groups = [{'Prompt Type': balanced_df['Prompt Type'].max()}]  # Example: largest group\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "import pandas as pd\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.algorithms.preprocessing import Reweighing\n",
    "\n",
    "# Load the dataset\n",
    "data_path = \"Processed_Most_Common_Term_By_Bias.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Drop unnecessary non-numerical columns\n",
    "df = df.drop(columns=['Original Prompt'], errors='ignore')  # Drop textual data\n",
    "\n",
    "# Encode categorical columns into numerical format\n",
    "df['Prompt Type'] = df['Prompt Type'].astype('category').cat.codes  # Encode 'Prompt Type'\n",
    "df['Most Common Term'] = df['Most Common Term'].astype('category').cat.codes  # Encode 'Most Common Term'\n",
    "\n",
    "# Ensure binary labels with at least one instance of each class\n",
    "most_frequent_term = df['Most Common Term'].mode()[0]  # Most frequent term\n",
    "df['Most Common Term'] = (df['Most Common Term'] == most_frequent_term).astype(int)\n",
    "\n",
    "# Balance each group within 'Prompt Type'\n",
    "balanced_groups = []\n",
    "for group_value in df['Prompt Type'].unique():\n",
    "    group = df[df['Prompt Type'] == group_value]\n",
    "    majority_class = group[group['Most Common Term'] == 0]\n",
    "    minority_class = group[group['Most Common Term'] == 1]\n",
    "\n",
    "    if len(minority_class) == 0:  # Add at least one favorable instance\n",
    "        continue\n",
    "    if len(majority_class) == 0:  # Add at least one unfavorable instance\n",
    "        continue\n",
    "\n",
    "    # Oversample the minority class within the group\n",
    "    minority_oversampled = resample(\n",
    "        minority_class,\n",
    "        replace=True,\n",
    "        n_samples=len(majority_class),\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Combine majority and oversampled minority\n",
    "    balanced_group = pd.concat([majority_class, minority_oversampled])\n",
    "    balanced_groups.append(balanced_group)\n",
    "\n",
    "# Combine all balanced groups\n",
    "balanced_df = pd.concat(balanced_groups).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Check the distribution\n",
    "print(balanced_df.groupby('Prompt Type')['Most Common Term'].value_counts())\n",
    "\n",
    "# Define unprivileged and privileged groups dynamically\n",
    "unprivileged_groups = [{'Prompt Type': balanced_df['Prompt Type'].min()}]\n",
    "privileged_groups = [{'Prompt Type': balanced_df['Prompt Type'].max()}]\n",
    "\n",
    "# Convert to BinaryLabelDataset\n",
    "dataset = BinaryLabelDataset(\n",
    "    df=balanced_df,\n",
    "    label_names=['Most Common Term'],\n",
    "    protected_attribute_names=['Prompt Type'],\n",
    "    favorable_label=1.0,\n",
    "    unfavorable_label=0.0\n",
    ")\n",
    "\n",
    "# Apply Reweighing\n",
    "reweigher = Reweighing(\n",
    "    unprivileged_groups=unprivileged_groups,\n",
    "    privileged_groups=privileged_groups\n",
    ")\n",
    "balanced_data = reweigher.fit_transform(dataset)\n",
    "\n",
    "# View transformed data\n",
    "balanced_data_df, _ = balanced_data.convert_to_dataframe()\n",
    "print(balanced_data_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(balanced_df.groupby('Prompt Type')['Most Common Term'].value_counts())\n",
    "print(balanced_data.instance_weights[:10])  # First 10 weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "\n",
    "metric = BinaryLabelDatasetMetric(\n",
    "    balanced_data,\n",
    "    unprivileged_groups=unprivileged_groups,\n",
    "    privileged_groups=privileged_groups\n",
    ")\n",
    "\n",
    "print(\"Mean Difference:\", metric.mean_difference())\n",
    "print(\"Disparate Impact:\", metric.disparate_impact())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Load the tokenizer and model from Hugging Face\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\", use_auth_token=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\", use_auth_token=True)\n",
    "\n",
    "# Save the tokenizer and model locally\n",
    "tokenizer.save_pretrained(\"./local_tokenizer\")\n",
    "model.save_pretrained(\"./local_model\")\n",
    "\n",
    "print(\"Tokenizer and model saved locally.\")\n",
    "import os\n",
    "\n",
    "print(\"Tokenizer files:\", os.listdir(\"./local_tokenizer\"))\n",
    "print(\"Model files:\", os.listdir(\"./local_model\"))\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Load tokenizer and model from the local directories\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./local_tokenizer\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./local_model\")\n",
    "\n",
    "print(\"Tokenizer and model loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Use pipeline for testing\n",
    "pipe = pipeline(\"text-generation\", model=\"./local_model\", tokenizer=\"./local_tokenizer\", device=0, torch_dtype=\"auto\")\n",
    "\n",
    "# Test with a sample prompt\n",
    "output = pipe(\"The goal of AI is\", max_length=50, num_return_sequences=1)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load the dataset\n",
    "csv_path = \"Processed_Most_Common_Term_By_Bias.csv\"\n",
    "data = pd.read_csv(csv_path)\n",
    "\n",
    "# Validate columns\n",
    "if 'Most Common Term' not in data.columns:\n",
    "    raise KeyError(\"The dataset must contain 'Most Common Term'.\")\n",
    "\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(data.head())\n",
    "\n",
    "# Step 2: Map 'Most Common Term' to numeric labels\n",
    "unique_labels = data['Most Common Term'].unique()\n",
    "label_mapping = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "\n",
    "# Add numeric labels for ground truth\n",
    "data['Numeric True Labels'] = data['Most Common Term'].map(label_mapping)\n",
    "\n",
    "# Step 3: Group by 'Prompt Type' (Optional Analysis)\n",
    "if 'Prompt Type' in data.columns:\n",
    "    grouped_data = data.groupby('Prompt Type').size()\n",
    "    print(\"\\nGrouped Data by 'Prompt Type':\")\n",
    "    print(grouped_data)\n",
    "else:\n",
    "    print(\"'Prompt Type' column not found. Skipping grouping analysis.\")\n",
    "\n",
    "# Step 4: Display Label Mapping and Data Summary\n",
    "print(\"\\nLabel Mapping:\")\n",
    "print(label_mapping)\n",
    "\n",
    "print(\"\\nDataset Summary:\")\n",
    "print(data.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_recall_fscore_support\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Load the dataset\n",
    "print(\"Loading the dataset...\")\n",
    "csv_path = \"Processed_Most_Common_Term_By_Bias.csv\"\n",
    "data = pd.read_csv(csv_path)\n",
    "\n",
    "# Validate columns\n",
    "required_columns = ['Original Prompt', 'Most Common Term']\n",
    "if not all(col in data.columns for col in required_columns):\n",
    "    raise KeyError(f\"The dataset must contain the following columns: {required_columns}\")\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(data.head())\n",
    "\n",
    "# Step 2: Initialize the LLM pipeline\n",
    "print(\"\\nInitializing the LLM pipeline...\")\n",
    "try:\n",
    "    pipe = pipeline(\"text-generation\", model=\"./local_model\", tokenizer=\"./local_tokenizer\", device=0, torch_dtype=\"auto\")\n",
    "    print(\"LLM pipeline initialized successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing pipeline: {e}\")\n",
    "    raise\n",
    "\n",
    "# Step 3: Initialize SentenceTransformer for semantic similarity\n",
    "print(\"\\nInitializing SentenceTransformer model for semantic similarity...\")\n",
    "try:\n",
    "    st_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    print(\"SentenceTransformer initialized successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing SentenceTransformer: {e}\")\n",
    "    raise\n",
    "\n",
    "# Step 4: Enhanced prediction generation\n",
    "def generate_prediction_with_explainability(prompt, max_retries=3):\n",
    "    \"\"\"Generates predictions and explanations with retries and fallback.\"\"\"\n",
    "    print(f\"Generating prediction for prompt: {prompt}\")\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            # Generate prediction\n",
    "            output = pipe(prompt, max_length=50, num_return_sequences=1)\n",
    "            generated_text = output[0]['generated_text']\n",
    "            \n",
    "            # Generate explanations\n",
    "            explanation = analyze_input_output_relationship(prompt, generated_text)\n",
    "            print(f\"Generated text: {generated_text}\")\n",
    "            print(f\"Explanation: {explanation}\")\n",
    "            return generated_text, explanation\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating prediction (attempt {retries + 1}): {e}\")\n",
    "            retries += 1\n",
    "    \n",
    "    # Fallback response\n",
    "    fallback_response = \"Unable to generate prediction. Default response applied.\"\n",
    "    print(f\"Fallback response used for prompt: {prompt}\")\n",
    "    return fallback_response, \"Explanation not available.\"\n",
    "\n",
    "def analyze_input_output_relationship(prompt, prediction):\n",
    "    \"\"\"Provides an explanation based on semantic similarity and token overlap.\"\"\"\n",
    "    if not prediction or prediction == \"INVALID\":\n",
    "        return \"No explanation available (invalid prediction).\"\n",
    "    \n",
    "    # Token overlap\n",
    "    prompt_tokens = set(prompt.lower().split())\n",
    "    prediction_tokens = set(prediction.lower().split())\n",
    "    matched_tokens = prompt_tokens & prediction_tokens\n",
    "    \n",
    "    # Semantic similarity\n",
    "    prompt_embedding = st_model.encode([prompt])\n",
    "    prediction_embedding = st_model.encode([prediction])\n",
    "    similarity_score = cosine_similarity(prompt_embedding, prediction_embedding)[0][0]\n",
    "    \n",
    "    explanation = f\"Semantic similarity: {similarity_score:.4f}. Matched tokens: {', '.join(matched_tokens)}\"\n",
    "    return explanation\n",
    "\n",
    "# Apply predictions to the dataset\n",
    "print(\"\\nGenerating predictions and explanations for all prompts...\")\n",
    "data[['Generated Predictions', 'Explanations']] = data['Original Prompt'].apply(\n",
    "    lambda prompt: generate_prediction_with_explainability(prompt)\n",
    ").apply(pd.Series)\n",
    "print(\"All predictions and explanations generated successfully!\")\n",
    "\n",
    "# Save the updated dataset\n",
    "output_csv_path = \"Dataset_With_Predictions_And_Explanations.csv\"\n",
    "data.to_csv(output_csv_path, index=False)\n",
    "print(f\"\\nPredictions and explanations added to the dataset and saved as '{output_csv_path}'.\")\n",
    "\n",
    "# Step 5: Map 'Most Common Term' to numeric labels\n",
    "print(\"\\nMapping 'Most Common Term' to numeric labels...\")\n",
    "unique_labels = data['Most Common Term'].unique()\n",
    "label_mapping = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "print(f\"Label mapping: {label_mapping}\")\n",
    "\n",
    "# Add numeric labels for ground truth\n",
    "data['Numeric True Labels'] = data['Most Common Term'].map(label_mapping)\n",
    "print(\"Numeric labels for ground truth added successfully!\")\n",
    "\n",
    "# Step 6: Map predictions to numeric labels\n",
    "def map_predictions(pred, label_mapping, default_label=-1):\n",
    "    \"\"\"Map predictions to numeric labels, with a fallback default.\"\"\"\n",
    "    print(f\"Mapping prediction: {pred}\")\n",
    "    if isinstance(pred, str):\n",
    "        pred_clean = pred.strip().lower()\n",
    "        for label, idx in label_mapping.items():\n",
    "            if label.lower() in pred_clean:\n",
    "                print(f\"Prediction '{pred}' matched label '{label}' with index {idx}\")\n",
    "                return idx\n",
    "    print(f\"No match found for prediction: {pred}. Using default label.\")\n",
    "    return default_label\n",
    "\n",
    "print(\"\\nMapping 'Generated Predictions' to numeric labels...\")\n",
    "data['Numeric Predictions'] = data['Generated Predictions'].apply(lambda x: map_predictions(x, label_mapping))\n",
    "print(\"Mapping of predictions completed successfully!\")\n",
    "\n",
    "# Step 7: Filter invalid predictions\n",
    "print(\"\\nFiltering invalid predictions...\")\n",
    "valid_data = data[data['Numeric Predictions'] != -1]\n",
    "ground_truth = valid_data['Numeric True Labels'].values\n",
    "predicted_labels = valid_data['Numeric Predictions'].values\n",
    "print(f\"Number of valid predictions: {len(predicted_labels)}\")\n",
    "\n",
    "# Step 8: Evaluate predictions\n",
    "if len(predicted_labels) > 0:\n",
    "    print(\"\\nEvaluating predictions...\")\n",
    "    # Accuracy and classification report\n",
    "    accuracy = accuracy_score(ground_truth, predicted_labels)\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    classification_report_output = classification_report(\n",
    "        ground_truth,\n",
    "        predicted_labels,\n",
    "        target_names=[str(label) for label in label_mapping.keys()],\n",
    "        zero_division=1\n",
    "    )\n",
    "    print(classification_report_output)\n",
    "    \n",
    "    # Precision, Recall, F1-Score Summary\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        ground_truth, predicted_labels, average='weighted'\n",
    "    )\n",
    "    print(f\"\\nPrecision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}\")\n",
    "\n",
    "    # Confusion matrix visualization\n",
    "    print(\"\\nGenerating confusion matrix...\")\n",
    "    cm = confusion_matrix(ground_truth, predicted_labels)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=label_mapping.keys(), yticklabels=label_mapping.keys())\n",
    "    plt.xlabel(\"Predicted Labels\")\n",
    "    plt.ylabel(\"True Labels\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No valid predictions available for evaluation.\")\n",
    "\n",
    "# Final Save\n",
    "final_output_csv = \"Final_Updated_Dataset_With_Explanations.csv\"\n",
    "data.to_csv(final_output_csv, index=False)\n",
    "print(f\"\\nFinal dataset with all columns saved as '{final_output_csv}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_recall_fscore_support\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Load the dataset\n",
    "print(\"Loading the dataset...\")\n",
    "csv_path = r\"C:\\Users\\rawan\\OneDrive\\Desktop\\thesisModify\\Final_Updated_Dataset_With_Explanations.csv\"\n",
    "data = pd.read_csv(csv_path)\n",
    "\n",
    "# Validate columns\n",
    "required_columns = ['Most Common Term', 'Generated Predictions']\n",
    "if not all(col in data.columns for col in required_columns):\n",
    "    raise KeyError(f\"The dataset must contain the following columns: {required_columns}\")\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(data.head())\n",
    "\n",
    "# Step 2: Map 'Most Common Term' and 'Generated Predictions' to numeric labels\n",
    "print(\"\\nMapping 'Most Common Term' and 'Generated Predictions' to numeric labels...\")\n",
    "unique_labels = data['Most Common Term'].unique()\n",
    "label_mapping = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "print(f\"Label mapping: {label_mapping}\")\n",
    "\n",
    "# Add numeric labels for ground truth and predictions\n",
    "data['Numeric True Labels'] = data['Most Common Term'].map(label_mapping)\n",
    "data['Numeric Predictions'] = data['Generated Predictions'].map(label_mapping)\n",
    "\n",
    "# Filter out rows with unmatched predictions\n",
    "valid_data = data.dropna(subset=['Numeric True Labels', 'Numeric Predictions'])\n",
    "ground_truth = valid_data['Numeric True Labels'].astype(int).values\n",
    "predicted_labels = valid_data['Numeric Predictions'].astype(int).values\n",
    "\n",
    "print(f\"Number of valid predictions: {len(predicted_labels)}\")\n",
    "\n",
    "# Step 3: Evaluate predictions\n",
    "if len(predicted_labels) > 0:\n",
    "    print(\"\\nEvaluating predictions...\")\n",
    "\n",
    "    # Accuracy and classification report\n",
    "    accuracy = accuracy_score(ground_truth, predicted_labels)\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    classification_report_output = classification_report(\n",
    "        ground_truth,\n",
    "        predicted_labels,\n",
    "        target_names=[str(label) for label in unique_labels],\n",
    "        zero_division=1\n",
    "    )\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report_output)\n",
    "\n",
    "    # Precision, Recall, F1-Score Summary\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        ground_truth, predicted_labels, average='weighted'\n",
    "    )\n",
    "    print(f\"\\nPrecision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}\")\n",
    "\n",
    "    # Step 4: Visualize confusion matrix\n",
    "    print(\"\\nGenerating confusion matrix...\")\n",
    "    cm = confusion_matrix(ground_truth, predicted_labels)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=unique_labels, yticklabels=unique_labels)\n",
    "    plt.xlabel(\"Predicted Labels\")\n",
    "    plt.ylabel(\"True Labels\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No valid predictions available for evaluation.\")\n",
    "\n",
    "# Step 5: Save updated dataset with numeric labels\n",
    "output_csv_path = r\"C:\\Users\\rawan\\OneDrive\\Desktop\\thesisModify\\Final_Updated_Dataset_With_Numeric_Labels.csv\"\n",
    "data.to_csv(output_csv_path, index=False)\n",
    "print(f\"\\nUpdated dataset with numeric labels saved to '{output_csv_path}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze misclassified examples\n",
    "print(\"\\nExtracting misclassified examples...\")\n",
    "misclassified = data[data['Numeric True Labels'] != data['Numeric Predictions']]\n",
    "misclassified_path = r\"C:\\Users\\rawan\\OneDrive\\Desktop\\thesisModify\\Misclassified_Examples.csv\"\n",
    "misclassified.to_csv(misclassified_path, index=False)\n",
    "print(f\"Misclassified examples saved to '{misclassified_path}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune semantic similarity threshold\n",
    "def evaluate_thresholds(thresholds, predictions, true_labels, label_mapping):\n",
    "    results = []\n",
    "    for threshold in thresholds:\n",
    "        print(f\"\\nEvaluating at threshold: {threshold}\")\n",
    "        numeric_predictions = map_predictions_with_similarity(predictions, true_labels, label_mapping, threshold)\n",
    "        valid_data = [pred for pred in numeric_predictions if pred != -1]\n",
    "        valid_indices = [i for i, pred in enumerate(numeric_predictions) if pred != -1]\n",
    "        ground_truth = data.loc[valid_indices, 'Numeric True Labels'].values\n",
    "        if len(valid_data) > 0:\n",
    "            accuracy = accuracy_score(ground_truth, valid_data)\n",
    "            print(f\"Accuracy at threshold {threshold}: {accuracy:.4f}\")\n",
    "            results.append((threshold, accuracy))\n",
    "        else:\n",
    "            print(f\"No valid predictions at threshold {threshold}.\")\n",
    "    return results\n",
    "\n",
    "# Define thresholds\n",
    "thresholds = [0.4, 0.5, 0.6, 0.7, 0.8]\n",
    "threshold_results = evaluate_thresholds(thresholds, data['Generated Predictions'], data['Most Common Term'], label_mapping)\n",
    "\n",
    "# Save threshold results\n",
    "threshold_results_path = r\"C:\\Users\\rawan\\OneDrive\\Desktop\\thesisModify\\Threshold_Results.csv\"\n",
    "pd.DataFrame(threshold_results, columns=['Threshold', 'Accuracy']).to_csv(threshold_results_path, index=False)\n",
    "print(f\"\\nThreshold evaluation results saved to '{threshold_results_path}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Step 1: Split the mitigated dataset into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = balanced_data_df.drop(columns=['Most Common Term'])\n",
    "y = balanced_data_df['Most Common Term']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Step 2: Train a model (Random Forest in this example)\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Step 3: Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Step 4: Evaluate performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Detailed performance metrics\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.metrics import demographic_parity_difference, equalized_odds_difference\n",
    "\n",
    "# Add sensitive feature information (Prompt Type) to test set\n",
    "sensitive_features_test = X_test['Prompt Type']  # Ensure this column is available in your dataset\n",
    "\n",
    "# Step 1: Calculate Demographic Parity Difference\n",
    "dp_diff = demographic_parity_difference(\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred,\n",
    "    sensitive_features=sensitive_features_test\n",
    ")\n",
    "print(f\"Demographic Parity Difference: {dp_diff:.4f}\")\n",
    "\n",
    "# Step 2: Calculate Equalized Odds Difference\n",
    "eo_diff = equalized_odds_difference(\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred,\n",
    "    sensitive_features=sensitive_features_test\n",
    ")\n",
    "print(f\"Equalized Odds Difference: {eo_diff:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aif360.datasets import BinaryLabelDataset\n",
    "\n",
    "# Convert training and testing datasets into AIF360 BinaryLabelDataset format\n",
    "dataset_train = BinaryLabelDataset(\n",
    "    df=X_train.assign(label=y_train),\n",
    "    label_names=['label'],\n",
    "    protected_attribute_names=['Prompt Type']\n",
    ")\n",
    "\n",
    "dataset_test = BinaryLabelDataset(\n",
    "    df=X_test.assign(label=y_test),\n",
    "    label_names=['label'],\n",
    "    protected_attribute_names=['Prompt Type']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aif360.algorithms.inprocessing import AdversarialDebiasing\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_eager_execution()\n",
    "\n",
    "# Define privileged and unprivileged groups\n",
    "unprivileged_groups = [{'Prompt Type': 0}]\n",
    "privileged_groups = [{'Prompt Type': 1}]\n",
    "\n",
    "# Train the model\n",
    "sess = tf.Session()\n",
    "adv_debiasing = AdversarialDebiasing(\n",
    "    privileged_groups=privileged_groups,\n",
    "    unprivileged_groups=unprivileged_groups,\n",
    "    scope_name='debiasing',\n",
    "    debias=True,\n",
    "    sess=sess\n",
    ")\n",
    "adv_debiasing.fit(dataset_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "predictions = adv_debiasing.predict(dataset_test)\n",
    "# Evaluate performance\n",
    "accuracy = accuracy_score(dataset_test.labels, predictions.labels)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Fairness metrics visualization\n",
    "metrics = [\"Demographic Parity Difference\", \"Equalized Odds Difference\"]\n",
    "values = [dp_diff, eo_diff]\n",
    "\n",
    "# Set up the bar chart\n",
    "plt.figure(figsize=(10, 6))  # Adjust size for better readability\n",
    "bars = plt.bar(metrics, values, color=['skyblue', 'coral'], edgecolor='black')\n",
    "\n",
    "# Add value annotations to each bar\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(\n",
    "        bar.get_x() + bar.get_width() / 2.0,  # Center the text horizontally\n",
    "        height + 0.02,  # Place above the bar\n",
    "        f\"{height:.4f}\",  # Format value to 4 decimal places\n",
    "        ha='center',  # Horizontal alignment\n",
    "        va='bottom',  # Vertical alignment\n",
    "        fontsize=10    # Font size\n",
    "    )\n",
    "\n",
    "# Set chart labels and title\n",
    "plt.title(\"Fairness Metrics\", fontsize=16)\n",
    "plt.ylabel(\"Metric Value\", fontsize=12)\n",
    "plt.ylim(0, max(values) + 0.1)  # Add padding above the bars\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)  # Add gridlines for y-axis\n",
    "plt.tight_layout()  # Adjust layout to prevent clipping\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Validate input data\n",
    "if 'Prompt Type' not in data.columns:\n",
    "    raise KeyError(\"The 'Prompt Type' column is missing from the dataset.\")\n",
    "\n",
    "# Step 2: Identify majority and minority classes dynamically\n",
    "majority_class = 3  # Replace with the actual majority class value if different\n",
    "majority = data[data['Prompt Type'] == majority_class]\n",
    "minority = data[data['Prompt Type'] != majority_class]\n",
    "\n",
    "print(f\"Majority class size: {len(majority)}\")\n",
    "print(f\"Minority class size: {len(minority)}\")\n",
    "\n",
    "# Step 3: Oversample the minority class\n",
    "print(\"Oversampling the minority class...\")\n",
    "minority_oversampled = resample(\n",
    "    minority,\n",
    "    replace=True,          # Sample with replacement\n",
    "    n_samples=len(majority),  # Match the majority class size\n",
    "    random_state=42        # Ensure reproducibility\n",
    ")\n",
    "\n",
    "# Step 4: Combine oversampled minority and majority classes\n",
    "balanced_data = pd.concat([majority, minority_oversampled])\n",
    "\n",
    "# Step 5: Shuffle the balanced dataset\n",
    "balanced_data = balanced_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Step 6: Print dataset summary\n",
    "print(f\"Balanced dataset size: {len(balanced_data)}\")\n",
    "print(balanced_data['Prompt Type'].value_counts())  # Verify class distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.algorithms.inprocessing import AdversarialDebiasing\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Convert data to AIF360 BinaryLabelDataset\n",
    "dataset = BinaryLabelDataset(\n",
    "    df=data,\n",
    "    label_names=['Most Common Term'],  # Replace with your label column\n",
    "    protected_attribute_names=['Prompt Type']  # Replace with your sensitive attribute\n",
    ")\n",
    "\n",
    "# Split data into training and testing datasets\n",
    "train_data, test_data = dataset.split([0.8], shuffle=True)\n",
    "\n",
    "# Create TensorFlow session\n",
    "sess = tf.Session()\n",
    "\n",
    "# Initialize Adversarial Debiasing model\n",
    "adv_debias = AdversarialDebiasing(\n",
    "    privileged_groups=[{'Prompt Type': 1}],  # Privileged group (e.g., group with higher advantage)\n",
    "    unprivileged_groups=[{'Prompt Type': 0}],  # Unprivileged group\n",
    "    scope_name=\"adversarial_debiasing\",\n",
    "    debias=True,\n",
    "    sess=sess\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "adv_debias.fit(train_data)\n",
    "\n",
    "# Predict on the test dataset\n",
    "predictions = adv_debias.predict(test_data)\n",
    "\n",
    "# Evaluate fairness metrics\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "metric = BinaryLabelDatasetMetric(\n",
    "    predictions,\n",
    "    unprivileged_groups=[{'Prompt Type': 0}],\n",
    "    privileged_groups=[{'Prompt Type': 1}]\n",
    ")\n",
    "print(f\"Mean Difference: {metric.mean_difference()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aif360.algorithms.preprocessing import Reweighing\n",
    "\n",
    "# Reweighing algorithm\n",
    "reweigher = Reweighing(\n",
    "    unprivileged_groups=[{'Prompt Type': 0}],  # Unprivileged group\n",
    "    privileged_groups=[{'Prompt Type': 1}]  # Privileged group\n",
    ")\n",
    "balanced_data = reweigher.fit_transform(dataset)\n",
    "\n",
    "# View instance weights\n",
    "print(balanced_data.instance_weights[:10])\n",
    "\n",
    "# Continue training with reweighted data...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load unseen data\n",
    "unseen_data = pd.read_csv(\"unseen_prompts.csv\")\n",
    "\n",
    "# Prepare features and sensitive attributes\n",
    "X_unseen = unseen_data['Original Prompt']  # Replace with appropriate feature columns\n",
    "sensitive_features_unseen = unseen_data['Prompt Type']  # Sensitive attribute\n",
    "\n",
    "# Tokenize unseen prompts\n",
    "inputs = tokenizer(\n",
    "    X_unseen.tolist(),\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# Generate predictions\n",
    "with torch.no_grad():\n",
    "    unseen_predictions = model.generate(\n",
    "        inputs['input_ids'].to(device),\n",
    "        max_length=50,\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "\n",
    "# Decode predictions\n",
    "unseen_data['Generated Predictions'] = [tokenizer.decode(p, skip_special_tokens=True) for p in unseen_predictions]\n",
    "\n",
    "# Evaluate fairness metrics\n",
    "dp_diff_unseen = demographic_parity_difference(\n",
    "    y_true=unseen_data['Most Common Term'],\n",
    "    y_pred=unseen_data['Generated Predictions'],  # Map predictions to numeric labels first\n",
    "    sensitive_features=sensitive_features_unseen\n",
    ")\n",
    "print(f\"Demographic Parity Difference (Unseen Data): {dp_diff_unseen:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.metrics import demographic_parity_difference, equalized_odds_difference\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Define the sensitive attribute types\n",
    "sensitive_attributes = ['Age Bias', 'Gender Bias', 'Race Bias', 'Ethnic Bias']\n",
    "\n",
    "# Create dictionaries to store results\n",
    "fairness_results = {\"Before Mitigation\": {}, \"After Mitigation\": {}}\n",
    "\n",
    "# Iterate through each sensitive attribute\n",
    "for sensitive_attr in sensitive_attributes:\n",
    "    print(f\"\\nEvaluating fairness for: {sensitive_attr}\")\n",
    "\n",
    "    # Ensure the sensitive attribute column exists\n",
    "    if sensitive_attr not in data.columns:\n",
    "        raise KeyError(f\"Sensitive attribute '{sensitive_attr}' is missing from the dataset.\")\n",
    "\n",
    "    # Extract sensitive features and ground truth labels\n",
    "    sensitive_features = data[sensitive_attr]\n",
    "    y_true = data['Most Common Term']  # Replace with your ground truth column\n",
    "\n",
    "    # Step 1: Pre-Mitigation Predictions (Baseline Model)\n",
    "    print(f\"Generating pre-mitigation predictions for {sensitive_attr}...\")\n",
    "    inputs = tokenizer(\n",
    "        data['Original Prompt'].tolist(),\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        baseline_predictions = model.generate(\n",
    "            inputs['input_ids'].to(device),\n",
    "            max_length=50,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    y_pred_pre = [tokenizer.decode(p, skip_special_tokens=True) for p in baseline_predictions]\n",
    "\n",
    "    # Map pre-mitigation predictions to numeric labels\n",
    "    def map_predictions(pred):\n",
    "        return 1 if \"positive\" in pred.lower() else 0  # Adjust logic as needed\n",
    "    y_pred_pre = list(map(map_predictions, y_pred_pre))\n",
    "\n",
    "    # Calculate fairness metrics before mitigation\n",
    "    dp_diff_pre = demographic_parity_difference(y_true=y_true, y_pred=y_pred_pre, sensitive_features=sensitive_features)\n",
    "    eo_diff_pre = equalized_odds_difference(y_true=y_true, y_pred=y_pred_pre, sensitive_features=sensitive_features)\n",
    "    print(f\"Pre-Mitigation Demographic Parity Difference: {dp_diff_pre:.4f}\")\n",
    "    print(f\"Pre-Mitigation Equalized Odds Difference: {eo_diff_pre:.4f}\")\n",
    "\n",
    "    # Store pre-mitigation results\n",
    "    fairness_results[\"Before Mitigation\"][sensitive_attr] = {\n",
    "        \"Demographic Parity Difference\": dp_diff_pre,\n",
    "        \"Equalized Odds Difference\": eo_diff_pre\n",
    "    }\n",
    "\n",
    "    # Step 2: Post-Mitigation Predictions (Fairness Optimized Model)\n",
    "    print(f\"Generating post-mitigation predictions for {sensitive_attr}...\")\n",
    "    adjusted_predictions = threshold_optimizer.predict(\n",
    "        X=inputs['input_ids'].cpu().numpy(),  # Pass tokenized inputs\n",
    "        sensitive_features=sensitive_features\n",
    "    )\n",
    "\n",
    "    # Calculate fairness metrics after mitigation\n",
    "    dp_diff_post = demographic_parity_difference(\n",
    "        y_true=y_true,\n",
    "        y_pred=adjusted_predictions,\n",
    "        sensitive_features=sensitive_features\n",
    "    )\n",
    "    eo_diff_post = equalized_odds_difference(\n",
    "        y_true=y_true,\n",
    "        y_pred=adjusted_predictions,\n",
    "        sensitive_features=sensitive_features\n",
    "    )\n",
    "    print(f\"Post-Mitigation Demographic Parity Difference: {dp_diff_post:.4f}\")\n",
    "    print(f\"Post-Mitigation Equalized Odds Difference: {eo_diff_post:.4f}\")\n",
    "\n",
    "    # Store post-mitigation results\n",
    "    fairness_results[\"After Mitigation\"][sensitive_attr] = {\n",
    "        \"Demographic Parity Difference\": dp_diff_post,\n",
    "        \"Equalized Odds Difference\": eo_diff_post\n",
    "    }\n",
    "\n",
    "# Save fairness results to JSON\n",
    "output_file = \"fairness_results_before_after.json\"\n",
    "with open(output_file, \"w\") as f:\n",
    "    json.dump(fairness_results, f, indent=4)\n",
    "\n",
    "print(f\"\\nFairness results saved to '{output_file}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Extract metrics for visualization\n",
    "metrics = [\"Demographic Parity\", \"Equalized Odds\"]\n",
    "for attr, results in fairness_results.items():\n",
    "    before = [results[\"Before Mitigation\"][attr][\"Demographic Parity Difference\"],\n",
    "              results[\"Before Mitigation\"][attr][\"Equalized Odds Difference\"]]\n",
    "    after = [results[\"After Mitigation\"][attr][\"Demographic Parity Difference\"],\n",
    "             results[\"After Mitigation\"][attr][\"Equalized Odds Difference\"]]\n",
    "    \n",
    "    # Plot metrics\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    x = np.arange(len(metrics))\n",
    "    bar_width = 0.4\n",
    "    plt.bar(x, before, width=bar_width, label=\"Before Mitigation\", color=\"blue\", alpha=0.7)\n",
    "    plt.bar(x + bar_width, after, width=bar_width, label=\"After Mitigation\", color=\"orange\", alpha=0.7)\n",
    "    plt.xticks(x + bar_width / 2, metrics, fontsize=12)\n",
    "    plt.ylabel(\"Metric Value\", fontsize=12)\n",
    "    plt.title(f\"Fairness Metrics: {attr}\", fontsize=14)\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aif360.metrics import ClassificationMetric\n",
    "\n",
    "# Convert predictions to BinaryLabelDataset\n",
    "balanced_data.labels = torch.tensor(ground_truths)\n",
    "balanced_data.scores = torch.tensor(predictions)\n",
    "\n",
    "# Evaluate fairness\n",
    "metric = ClassificationMetric(\n",
    "    balanced_data,\n",
    "    unprivileged_groups=[{'Prompt Type': 0}],  # Adjust for your data\n",
    "    privileged_groups=[{'Prompt Type': 3}]\n",
    ")\n",
    "print(\"Demographic Parity Difference:\", metric.mean_difference())\n",
    "print(\"Equalized Odds Difference:\", metric.equalized_odds_difference())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaForSequenceClassification, AdamW\n",
    "\n",
    "# Load LLaMA model\n",
    "model = LlamaForSequenceClassification.from_pretrained('llama-3.2-1B', num_labels=2)  # Binary classification\n",
    "\n",
    "# Set the model to training mode\n",
    "model.train()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Fine-tune the model\n",
    "from tqdm import tqdm\n",
    "\n",
    "epochs = 3  # Number of epochs to train\n",
    "for epoch in range(epochs):\n",
    "    loop = tqdm(data_loader, leave=True)\n",
    "    for batch in loop:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update progress bar\n",
    "        loop.set_description(f'Epoch {epoch}')\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained('./fine_tuned_llama')\n",
    "tokenizer.save_pretrained('./fine_tuned_llama')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aif360.metrics import ClassificationMetric\n",
    "\n",
    "# Create predictions\n",
    "model.eval()\n",
    "predictions = []\n",
    "ground_truths = []\n",
    "for batch in data_loader:\n",
    "    input_ids, attention_mask, labels = batch\n",
    "    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    preds = torch.argmax(outputs.logits, axis=1)\n",
    "    predictions.extend(preds.tolist())\n",
    "    ground_truths.extend(labels.tolist())\n",
    "\n",
    "# Convert predictions and ground_truths to BinaryLabelDataset format\n",
    "balanced_data.labels = torch.tensor(ground_truths)\n",
    "balanced_data.scores = torch.tensor(predictions)\n",
    "\n",
    "# Evaluate fairness metrics\n",
    "metric = ClassificationMetric(\n",
    "    balanced_data,\n",
    "    unprivileged_groups=unprivileged_groups,\n",
    "    privileged_groups=privileged_groups\n",
    ")\n",
    "print(\"Demographic Parity Difference:\", metric.mean_difference())\n",
    "print(\"Equalized Odds Difference:\", metric.equalized_odds_difference())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.postprocessing import ThresholdOptimizer\n",
    "\n",
    "# Split the dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    balanced_data_df['text_column'],\n",
    "    balanced_data_df['Most Common Term'],\n",
    "    test_size=0.3,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Tokenize the test set\n",
    "test_inputs = tokenizer(\n",
    "    X_test.tolist(),\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "# Apply Threshold Optimizer\n",
    "threshold_optimizer = ThresholdOptimizer(\n",
    "    estimator=model,\n",
    "    constraints=\"equalized_odds\"\n",
    ")\n",
    "threshold_optimizer.fit(\n",
    "    X_train=inputs['input_ids'],\n",
    "    y_train=labels,\n",
    "    sensitive_features=balanced_data_df['Prompt Type'][:len(labels)]\n",
    ")\n",
    "\n",
    "# Adjust predictions\n",
    "adjusted_predictions = threshold_optimizer.predict(\n",
    "    X=test_inputs['input_ids'],\n",
    "    sensitive_features=balanced_data_df['Prompt Type'][len(labels):]\n",
    ")\n",
    "\n",
    "print(\"Adjusted Predictions:\", adjusted_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Fairness metrics before and after\n",
    "metrics = ['Demographic Parity Difference', 'Equalized Odds Difference']\n",
    "values_before = [metric.mean_difference(), metric.equalized_odds_difference()]\n",
    "values_after = [0.0, 0.0]  # Update with post-processed metrics\n",
    "\n",
    "# Create bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(metrics, values_before, label='Before Post-Processing', alpha=0.7, color='skyblue')\n",
    "plt.bar(metrics, values_after, label='After Post-Processing', alpha=0.7, color='orange')\n",
    "plt.title('Fairness Metrics Comparison')\n",
    "plt.ylabel('Metric Value')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
